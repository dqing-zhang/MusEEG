<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="The Emotiv^®^ EPOC+Although easily modifiable, the MusEEG package is designed to work withthe Emotiv^®^ EPOC+. The EPOC+ is an affordable commercial-grade EE...">
  <meta name="keywords" content="blog and jekyll">
  <meta name="author" content="Data Acquisition, Preprocessing, and Training | hugofloresgarcia/museeg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#f5f5f5">

  <!-- Twitter Tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Data Acquisition, Preprocessing, and Training | hugofloresgarcia/museeg">
  <meta name="twitter:description" content="The Emotiv^®^ EPOC+Although easily modifiable, the MusEEG package is designed to work withthe Emotiv^®^ EPOC+. The EPOC+ is an affordable commercial-grade EE...">
  
    <meta property="twitter:image" content="http://localhost:4000/img/leonids-logo.png">
  

  <!-- Open Graph Tags -->
  <meta property="og:type" content="blog">
  <meta property="og:url" content="http://localhost:4000/documentation/0_data_acquisition">
  <meta property="og:title" content="Data Acquisition, Preprocessing, and Training | hugofloresgarcia/museeg">
  <meta property="og:description" content="The Emotiv^®^ EPOC+Although easily modifiable, the MusEEG package is designed to work withthe Emotiv^®^ EPOC+. The EPOC+ is an affordable commercial-grade EE...">
  
    <meta property="og:image" content="http://localhost:4000/img/leonids-logo.png">
  
  <title>Data Acquisition, Preprocessing, and Training | hugofloresgarcia/museeg</title>

  <!-- CSS files -->
  <link rel="stylesheet" href="http://localhost:4000/css/font-awesome.min.css">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">

  <link rel="canonical" href="http://localhost:4000/documentation/0_data_acquisition">
  <link rel="alternate" type="application/rss+xml" title="hugofloresgarcia/museeg" href="http://localhost:4000/feed.xml" />

  <!-- Icons -->
  <!-- 16x16 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
  <!-- 32x32 -->
  <link rel="shortcut icon" href="http://localhost:4000/favicon.png">
</head>


<body>
  <div class="row">
    <div class="col s12 m4">
      <div class="table cover">
        

<div class="cover-card table-cell table-middle">
  
  <a href="http://localhost:4000/">
    <img src="http://localhost:4000/img/flower_logo_v1_cropped.png" alt="" class="avatar">
  </a>
  
  <a href="http://localhost:4000/" class="author_name">hugofloresgarcia/museeg</a>
  <span class="author_job"></span>
  <span class="author_bio mbm">a cool open source brain-computer interface for music performance</span>
  <nav class="nav">
    <ul class="nav-list">
      <li class="nav-item">
        <a href="https://hugofloresgarcia.github.io/">home (hugofloresgarcia)</a>
      </li>
      <li class="nav-item">
        <a href="http://localhost:4000/">about</a>
      </li>
       
      <li class="nav-item">
        <a href="http://localhost:4000/pages/deliverables">deliverables</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/pages/demo">demo</a>
      </li>
        
      <li class="nav-item">
        <a href="http://localhost:4000/documentation/">documentation</a>
      </li>
               
    </ul>
  </nav>
  <script type="text/javascript">
  // based on http://stackoverflow.com/a/10300743/280842
  function gen_mail_to_link(hs, subject) {
    var lhs,rhs;
    var p = hs.split('@');
    lhs = p[0];
    rhs = p[1];
    document.write("<a class=\"social-link-item\" target=\"_blank\" href=\"mailto");
    document.write(":" + lhs + "@");
    document.write(rhs + "?subject=" + subject + "\"><i class=\"fa fa-fw fa-envelope\"></i><\/a>");
  }
</script>
<div class="social-links">
  <ul>
    
      <li>
      <script>gen_mail_to_link('hf01049@georgiasouthern.edu', 'Hello from website');</script>
      </li>
    
    
    
    
    
    
    
    
    
    <li><a href="http://github.com/hugofloresgarcia/museeg" class="social-link-item" target="_blank"><i class="fa fa-fw fa-github"></i></a></li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </ul>
</div>

</div>

      </div>
    </div>
    <div class="col s12 m8">
      <div class="post-listing">
          <div id="page">
    <header class="page-header">
      <h2>Data Acquisition, Preprocessing, and Training</h2>
    </header>

    <article class="page-content">
      <h2 id="the-emotiv-epoc">The Emotiv^®^ EPOC+</h2>

<p><img src="epoc.png" alt="Emotiv EPOC+ Headset" /></p>

<p>Although easily modifiable, the MusEEG package is designed to work with
the Emotiv^®^ EPOC+. The EPOC+ is an affordable commercial-grade EEG
headset that records 14 EEG channels and samples at a rate of 256Hz with
14-bit resolution, with a least significant bit value of approximately
0.51 $\mu$V. The EPOC+ headset proves to be an economically feasible
alternative to a medical-grade headset as various studies confirm its
viability for noncritical applications.</p>

<h2 id="data-acquisition">Data Acquisition</h2>

<p>To maximize classification accuracy, the MusEEG package is designed with
a train-it-yourself structure, meaning that the end-user will have to
train their own ANN model to work with their preferred set of facial or
body expressions. Because of the train-it-yourself nature of the
package, data was recorded for a single subject only. Six facial
expressions were recorded using the Emotiv^®^ PRO application. 80
samples were recorded of the following facial expressions:</p>

<ul>
  <li>
    <p>smile</p>
  </li>
  <li>
    <p>raise eyebrows</p>
  </li>
  <li>
    <p>look left</p>
  </li>
  <li>
    <p>look right</p>
  </li>
  <li>
    <p>neutral</p>
  </li>
  <li>
    <p>scrunch</p>
  </li>
</ul>

<p>To expedite recording times, the MusEEG package was designed to allow
the user to record all samples of a single facial expression to a single
.csv file. The MusEEG package then aids the user through curating and
cutting the samples into individual chunks for feature extraction and
classification.</p>

<p><img src="smileBigChunk_320.png" alt="EEG plot of a smile sample
(*bigChunk*)" /></p>

<p>During the curation process, the samples were examined for any
discontinuities in contact quality, noise, and other artifacts. Samples
that appeared to be corrupted with electrode contact discontinuities
were discarded, while samples that were clean were stored in the
project’s saved chunks directory. From each sample, two different sample
chunks were created: a <em>bigChunk</em> (1250ms, 320 samples) for facial
expression recognition, and a <em>smallChunk</em> (250ms, 64 samples) for
facial expression/no facial expression classification. Examples of a
<em>bigChunk</em> and <em>smallChunk</em> can be observed in
(Figures <a href="#fig:smileBigChunk">3.2</a> and
<a href="#fig:smileSmallChunk">3.3</a>), respectively.</p>

<p><img src="smileSmallChunk_64.png" alt="EEG plot of a smile sample
(*smallChunk*)" /></p>

<h2 id="feature-extraction">Feature Extraction</h2>

<h3 id="facial-expression-feature-extraction">Facial Expression Feature Extraction</h3>

<p>A feature extraction method similar to the one in [@14] was used. A
4-level wavelet decomposition using Daubechies order-2 (db2) mother
wavelet is performed on all 14 channels of the chunk of EEG data.</p>

<p>One advantage of wavelet analysis over other time-frequency distribution
methods (e.g. STFT) is that wavelet analysis varies the time-frequency
aspect ratio, producing good frequency localization at low frequencies
(long time windows) and good time localization at high frequencies
(short time windows). This results in a segmentation of the
time-frequency plane that will reveal transient features of the signal,
which are typically not obvious during Fourier analysis [@14].</p>

<p>Following the wavelet decomposition, the first four statistical moments
(mean, variance, skewness, kurtosis) are calculated for each wavelet
vector. Since four moments are calculated for each of the 5 wavelet
decomposition vectors per EEG channel, a total of 14 x 4 x 5 (280)
features are calculated. The data is then normalized using <em>sklearn</em>’s
MinMaxScaler. These features are used as an input for the classification
models.</p>

<p><img src="smileWavelets.png" alt="Wavelet coefficients plot for
smile" /></p>

<h3 id="band-power-feature-extraction">Band Power Feature Extraction</h3>

<p>The MusEEG system is capable of sending continuous theta (4 - 8 Hz),
alpha (8 - 12 Hz), beta (12 - 30 Hz), and gamma (30 - 60 Hz) band power
information at a rate of 2Hz.</p>

<p><img src="bandPower.png" alt="EEG band power during neutral state" /></p>

<p>Before the average band power is calculated, the power spectral density
of all of the 14-channel data is calculated using Welch’s periodogram,
with a window duration of 0.5s. Once the power spectral density is
calculated, the average band power over a certain range of frequencies
is found by integrating the power spectral density over the frequency
ranges.</p>

<h2 id="classification">Classification</h2>

<p>A three-layer Artificial Neural Network (ANN) architecture was used for
the classification of the EEG signals. Two models were created: a small
model designed to analyze 64-sample chunks and determine whether the
chunk contains a facial expression or not (<em>smallBrain</em>), and a large
model designed to analyze 320-sample chunks to determine what facial
expression is present in the chunk (<em>bigBrain</em>).</p>

<p>The <em>smallBrain</em> model is meant to perform a quick classification of
whether a facial expression is present in a 64-sample chunk. The
<em>smallBrain</em> model was designed with the intent of having an always-on
processor for real-time implementation. By analyzing a chunk with a
duration of 250ms, the <em>smallBrain</em> model returns a result fairly
quickly, allowing for continuous real-time classification with little
latency.</p>

<p><img src="smallBrain.png" alt="ANN architecture for *smallBrain*
model." /></p>

<p><img src="smallBrainConfusion.png" alt="Confusion Matrix for *smallBrain*
model." /></p>

<p>The <em>smallBrain</em> model consists of a three-layer ANN with L1
regularization methods and a sparse categorical cross entropy loss
function. During testing, the model exhibited 98.75% accuracy. Its test
confusion matrix can be observed in
(Figure <a href="#fig:smallBrainConfusion">3.7</a>) .</p>

<p>The <em>bigBrain</em> model consists of a three-layer ANN with L1_L2
regularization methods and a sparse categorical cross entropy loss
function. During testing, the model exhibited 87.04% accuracy. Its test
confusion matrix can be observed in
(Figure <a href="#fig:bigBrainConfusion">3.9</a>).</p>

<p><img src="bigBrain.png" alt="ANN Architecture for *bigBrain* model." /></p>

<p><img src="bigBrainConfusion.png" alt="Confusion Matrix for *bigBrain*
model." /></p>

    </article>


  </div><!-- end page content -->


  

        <footer>
  &copy; 2020 hugofloresgarcia/museeg. Powered by <a href="http://jekyllrb.com/">Jekyll</a>, <a href="http://github.com/renyuanz/leonids/">leonids theme</a> made with <i class="fa fa-heart heart-icon"></i>
</footer>

      </div>
    </div>
  </div>
  <script type="text/javascript" src="http://localhost:4000/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="http://localhost:4000/js/main.js"></script>


</body>
</html>

An open-source, facial expression brain-computer music interface (BCMI) for music performance is proposed. After a user performs a pre-defined facial expression, the facial expression is classified using feature extraction and machine learning methods. Raw EEG data is streamed using the commercial grade EmotivÂ® EPOC+ EEG headset, preprocessed using a 4-level wavelet decomposition, and classified using an artificial neural network (ANN) classifier. Each facial expression is mapped to a discrete MIDI or Open Sound Control (OSC) event. The system is capable of sending discrete facial expression commands as well as continuous EEG band power data, which can be used to modify musical control parameters. By using a commercial-grade headset and a workflow that is easy to comprehend, MusEEG aims to provide people with motor disabilities and various levels of music creation and/or performance experience with an opportunity to make music.